# -*- coding: utf-8 -*-
"""
@author: FinAI-Chat
@file: main.py
@time: 2025-06-30 11:00
@desc: DeepReader Agent çš„ä¸»ç¨‹åºå…¥å£
"""


import asyncio
import os
import sys
import json
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from pprint import pprint
from typing import Dict, Any, List, Optional

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# --- 1. åˆå§‹åŒ–ç¯å¢ƒ ---
def setup_environment():
    """è®¾ç½®å·¥ä½œç›®å½•å’Œ sys.pathï¼Œç¡®ä¿è„šæœ¬ä» dynamic-gptr æ ¹ç›®å½•è¿è¡Œ"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•
    script_dir = Path(__file__).parent.resolve()
    # å¯»æ‰¾ dynamic-gptr æ ¹ç›®å½•
    workspace_root = script_dir
    while workspace_root.name != 'dynamic-gptr' and workspace_root.parent != workspace_root:
        workspace_root = workspace_root.parent
    
    if workspace_root.name == 'dynamic-gptr':
        os.chdir(workspace_root)
        print(f"å·¥ä½œç›®å½•å·²åˆ‡æ¢åˆ°: {os.getcwd()}")
    else:
        print("é”™è¯¯: æœªèƒ½åœ¨çˆ¶ç›®å½•ä¸­æ‰¾åˆ° 'dynamic-gptr'ã€‚è¯·ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®ã€‚")
        sys.exit(1)

    # å°†å·¥ä½œç›®å½•æ·»åŠ åˆ° sys.path
    if str(workspace_root) not in sys.path:
        sys.path.insert(0, str(workspace_root))

# setup_environment()

# --- 2. å¯¼å…¥å¿…è¦çš„æ¨¡å— ---
from backend.read_graph import create_deepreader_graph
from backend.read_state import DeepReaderState
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# --- 3. å®šä¹‰å¸¸é‡ ---
# åŸºäº main.py æ–‡ä»¶æ‰€åœ¨ç›®å½•å®šä¹‰è·¯å¾„ï¼Œç¡®ä¿è¾“å‡ºç›®å½•æ­£ç¡®
SCRIPT_DIR = Path(__file__).parent.resolve()
BASE_OUTPUT_DIR = SCRIPT_DIR / "output"
CACHE_DIR = SCRIPT_DIR / "backend/cache" 
SESSION_CACHE_FILE = CACHE_DIR / "session_cache.json"
CHECKPOINTER_DB_PATH = CACHE_DIR / "checkpoints.sqlite"

# --- 4. é…ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# è¿‡æ»¤æ‰ä¸€äº›è¿‡äºå†—é•¿çš„ç¬¬ä¸‰æ–¹åº“æ—¥å¿—
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# --- 5. ä¼šè¯ç®¡ç† ---
def load_session_cache() -> Dict[str, str]:
    """åŠ è½½ä¸Šä¸€æ¬¡çš„ç”¨æˆ·è¾“å…¥"""
    if SESSION_CACHE_FILE.exists():
        try:
            with open(SESSION_CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}
    return {}

def save_session_cache(data: Dict[str, str]):
    """ä¿å­˜å½“å‰ç”¨æˆ·è¾“å…¥"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    with open(SESSION_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def get_user_inputs(defaults: Dict[str, str]) -> Dict[str, str]:
    """æç¤ºç”¨æˆ·è¾“å…¥å¹¶è·å–å¿…è¦çš„å‚æ•°"""
    print("\\n--- è¯·è¾“å…¥ç ”ç©¶ä»»åŠ¡æ‰€éœ€ä¿¡æ¯ ---")
    
    document_path = input(f"è¯·è¾“å…¥å¾…å¤„ç†æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ [{defaults.get('document_path', '')}]: ") or defaults.get('document_path', '')
    while not Path(document_path).exists() or not Path(document_path).is_file():
        print("âŒ æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
        document_path = input("è¯·è¾“å…¥å¾…å¤„ç†æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ")

    user_core_question = input(f"è¯·è¾“å…¥æ‚¨çš„æ ¸å¿ƒæ¢ç´¢é—®é¢˜ [{defaults.get('user_core_question', '')}]: ") or defaults.get('user_core_question', '')
    research_role = input(f"è¯·è¾“å…¥æ‚¨æœŸæœ›çš„ç ”ç©¶è§’è‰² [{defaults.get('research_role', 'èµ„æ·±è¡Œä¸šåˆ†æå¸ˆ')}]: ") or defaults.get('research_role', 'èµ„æ·±è¡Œä¸šåˆ†æå¸ˆ')

    return {
        "document_path": document_path,
        "user_core_question": user_core_question,
        "research_role": research_role
    }


def convert_document_to_markdown(file_path: str) -> str:
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹å°†æ–‡æ¡£è½¬æ¢ä¸º Markdown æ ¼å¼
    
    Args:
        file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        
    Returns:
        è½¬æ¢åçš„ Markdown å†…å®¹
    """
    from backend.scraper.pdf_converter import convert_pdf_to_markdown
    from backend.scraper.epub_converter import convert_epub_to_markdown
    
    file_path_obj = Path(file_path)
    file_ext = file_path_obj.suffix.lower()
    
    if file_ext == '.md':
        # å¦‚æœå·²ç»æ˜¯ Markdown æ–‡ä»¶ï¼Œç›´æ¥è¯»å–
        logging.info(f"æ£€æµ‹åˆ° Markdown æ–‡ä»¶ï¼Œç›´æ¥è¯»å–: {file_path}")
        return file_path_obj.read_text(encoding='utf-8')
    
    elif file_ext == '.pdf':
        # è½¬æ¢ PDF æ–‡ä»¶
        logging.info(f"æ£€æµ‹åˆ° PDF æ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢: {file_path}")
        
        # marker ä¼šåˆ›å»ºä¸€ä¸ªä»¥æ–‡ä»¶åå‘½åçš„ç›®å½•ï¼Œç„¶ååœ¨é‡Œé¢ç”Ÿæˆ markdown æ–‡ä»¶
        expected_md_dir = file_path_obj.parent / file_path_obj.stem
        expected_md_path = expected_md_dir / f"{file_path_obj.stem}.md"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è½¬æ¢åçš„æ–‡ä»¶
        if expected_md_path.exists():
            choice = input(f"\\nå‘ç°å·²å­˜åœ¨çš„ Markdown æ–‡ä»¶: {expected_md_path}\\næ˜¯å¦ä½¿ç”¨ç°æœ‰æ–‡ä»¶? (Y/n): ").lower()
            if choice == 'y' or choice == '':
                return expected_md_path.read_text(encoding='utf-8')
        
        # æ‰§è¡Œè½¬æ¢
        markdown_content = convert_pdf_to_markdown(file_path)
        
        # æŸ¥æ‰¾å®é™…ç”Ÿæˆçš„ markdown æ–‡ä»¶
        actual_md_path = None
        if expected_md_path.exists():
            actual_md_path = expected_md_path
            logging.info(f"æ‰¾åˆ°é¢„æœŸè·¯å¾„çš„ Markdown æ–‡ä»¶: {actual_md_path}")
        else:
            # å¦‚æœé¢„æœŸè·¯å¾„ä¸å­˜åœ¨ï¼Œæœç´¢å¯èƒ½çš„ä½ç½®
            search_locations = [
                file_path_obj.with_suffix('.md'),  # åŒç›®å½•ä¸‹çš„ç›´æ¥æ›¿æ¢
                expected_md_path,  # å­ç›®å½•ä¸­çš„é¢„æœŸä½ç½®
                file_path_obj.parent,  # çˆ¶ç›®å½•ä¸­æœç´¢
                expected_md_dir,  # å­ç›®å½•ä¸­æœç´¢
            ]
            
            logging.info(f"åœ¨é¢„æœŸè·¯å¾„æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œå¼€å§‹æœç´¢å…¶ä»–ä½ç½®...")
            
            for search_location in search_locations:
                if search_location.is_file() and search_location.suffix == '.md':
                    # ç›´æ¥æ˜¯ä¸€ä¸ª .md æ–‡ä»¶
                    if search_location.stem == file_path_obj.stem:
                        actual_md_path = search_location
                        logging.info(f"æ‰¾åˆ°åŒ¹é…çš„ Markdown æ–‡ä»¶: {actual_md_path}")
                        break
                elif search_location.is_dir():
                    # åœ¨ç›®å½•ä¸­æœç´¢ .md æ–‡ä»¶
                    md_files = list(search_location.glob("*.md"))
                    if md_files:
                        # ä¼˜å…ˆé€‰æ‹©ä¸åŸæ–‡ä»¶ååŒ¹é…çš„
                        for md_file in md_files:
                            if md_file.stem == file_path_obj.stem:
                                actual_md_path = md_file
                                logging.info(f"åœ¨ç›®å½• {search_location} ä¸­æ‰¾åˆ°åŒ¹é…çš„ Markdown æ–‡ä»¶: {actual_md_path}")
                                break
                        # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª .md æ–‡ä»¶
                        if not actual_md_path and md_files:
                            actual_md_path = md_files[0]
                            logging.info(f"åœ¨ç›®å½• {search_location} ä¸­æ‰¾åˆ° Markdown æ–‡ä»¶ï¼ˆéå®Œå…¨åŒ¹é…ï¼‰: {actual_md_path}")
                        break
        
        if not actual_md_path or not actual_md_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°è½¬æ¢åçš„ Markdown æ–‡ä»¶ã€‚é¢„æœŸä½ç½®: {expected_md_path}")
        
        print(f"\\nâœ… PDF è½¬æ¢å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {actual_md_path}")
        
        # æç¤ºç”¨æˆ·æ£€æŸ¥å’Œæ¸…ç†
        print("\\nâš ï¸  è¯·æ£€æŸ¥ç”Ÿæˆçš„ Markdown æ–‡ä»¶å¹¶è¿›è¡Œå¿…è¦çš„æ¸…ç†ï¼š")
        print("   - åˆ é™¤ä¸ç›¸å…³çš„å†…å®¹ï¼ˆå¦‚é™„å½•ã€å£°æ˜ç­‰ï¼‰")
        print("   - æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("   - ç¡®ä¿ç« èŠ‚ç»“æ„æ¸…æ™°")
        
        input("\\nè¯·å®Œæˆæ–‡ä»¶æ¸…ç†åæŒ‰å›è½¦é”®ç»§ç»­...")
        
        # é‡æ–°è¯»å–å¯èƒ½è¢«ç”¨æˆ·ä¿®æ”¹çš„æ–‡ä»¶
        return actual_md_path.read_text(encoding='utf-8')
        
    elif file_ext == '.epub':
        # è½¬æ¢ EPUB æ–‡ä»¶
        logging.info(f"æ£€æµ‹åˆ° EPUB æ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢: {file_path}")
        
        # EPUB ä¹Ÿè¾“å‡ºåˆ°å­æ–‡ä»¶å¤¹ï¼Œä¸ PDF ä¿æŒä¸€è‡´
        expected_md_dir = file_path_obj.parent / file_path_obj.stem
        expected_md_path = expected_md_dir / f"{file_path_obj.stem}.md"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è½¬æ¢åçš„æ–‡ä»¶
        if expected_md_path.exists():
            choice = input(f"\\nå‘ç°å·²å­˜åœ¨çš„ Markdown æ–‡ä»¶: {expected_md_path}\\næ˜¯å¦ä½¿ç”¨ç°æœ‰æ–‡ä»¶? (Y/n): ").lower()
            if choice == 'y' or choice == '':
                return expected_md_path.read_text(encoding='utf-8')
        
        # æ‰§è¡Œè½¬æ¢
        markdown_content = convert_epub_to_markdown(file_path)
        
        # æŸ¥æ‰¾å®é™…ç”Ÿæˆçš„ markdown æ–‡ä»¶  
        actual_md_path = None
        if expected_md_path.exists():
            actual_md_path = expected_md_path
            logging.info(f"æ‰¾åˆ°é¢„æœŸè·¯å¾„çš„ Markdown æ–‡ä»¶: {actual_md_path}")
        else:
            # å¦‚æœé¢„æœŸè·¯å¾„ä¸å­˜åœ¨ï¼Œæœç´¢å¯èƒ½çš„ä½ç½®
            search_locations = [
                file_path_obj.with_suffix('.md'),  # åŒç›®å½•ä¸‹çš„ç›´æ¥æ›¿æ¢
                expected_md_path,  # å­ç›®å½•ä¸­çš„é¢„æœŸä½ç½®
                file_path_obj.parent,  # çˆ¶ç›®å½•ä¸­æœç´¢
                expected_md_dir,  # å­ç›®å½•ä¸­æœç´¢
            ]
            
            logging.info(f"åœ¨é¢„æœŸè·¯å¾„æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œå¼€å§‹æœç´¢å…¶ä»–ä½ç½®...")
            
            for search_location in search_locations:
                if search_location.is_file() and search_location.suffix == '.md':
                    # ç›´æ¥æ˜¯ä¸€ä¸ª .md æ–‡ä»¶
                    if search_location.stem == file_path_obj.stem:
                        actual_md_path = search_location
                        logging.info(f"æ‰¾åˆ°åŒ¹é…çš„ Markdown æ–‡ä»¶: {actual_md_path}")
                        break
                elif search_location.is_dir():
                    # åœ¨ç›®å½•ä¸­æœç´¢ .md æ–‡ä»¶
                    md_files = list(search_location.glob("*.md"))
                    if md_files:
                        # ä¼˜å…ˆé€‰æ‹©ä¸åŸæ–‡ä»¶ååŒ¹é…çš„
                        for md_file in md_files:
                            if md_file.stem == file_path_obj.stem:
                                actual_md_path = md_file
                                logging.info(f"åœ¨ç›®å½• {search_location} ä¸­æ‰¾åˆ°åŒ¹é…çš„ Markdown æ–‡ä»¶: {actual_md_path}")
                                break
                        # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª .md æ–‡ä»¶
                        if not actual_md_path and md_files:
                            actual_md_path = md_files[0]
                            logging.info(f"åœ¨ç›®å½• {search_location} ä¸­æ‰¾åˆ° Markdown æ–‡ä»¶ï¼ˆéå®Œå…¨åŒ¹é…ï¼‰: {actual_md_path}")
                        break
        
        if not actual_md_path or not actual_md_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°è½¬æ¢åçš„ Markdown æ–‡ä»¶ã€‚é¢„æœŸä½ç½®: {expected_md_path}")
        
        print(f"\\nâœ… EPUB è½¬æ¢å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {actual_md_path}")
        
        # æç¤ºç”¨æˆ·æ£€æŸ¥å’Œæ¸…ç†
        print("\\nâš ï¸  è¯·æ£€æŸ¥ç”Ÿæˆçš„ Markdown æ–‡ä»¶å¹¶è¿›è¡Œå¿…è¦çš„æ¸…ç†ï¼š")
        print("   - åˆ é™¤ä¸ç›¸å…³çš„å†…å®¹ï¼ˆå¦‚ç›®å½•ã€ç‰ˆæƒä¿¡æ¯ç­‰ï¼‰")
        print("   - æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("   - ç¡®ä¿ç« èŠ‚ç»“æ„æ¸…æ™°")
        
        input("\\nè¯·å®Œæˆæ–‡ä»¶æ¸…ç†åæŒ‰å›è½¦é”®ç»§ç»­...")
        
        # é‡æ–°è¯»å–å¯èƒ½è¢«ç”¨æˆ·ä¿®æ”¹çš„æ–‡ä»¶
        return actual_md_path.read_text(encoding='utf-8')
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼: .md, .pdf, .epub")

# --- 6. ç»“æœæ ¼å¼åŒ–ä¸ä¿å­˜ ---
def _format_summaries_to_md(summaries: Dict[str, str]) -> str:
    """æ ¼å¼åŒ–ç« èŠ‚æ‘˜è¦ä¸º Markdown"""
    if not summaries:
        return "æ²¡æœ‰å¯ç”¨çš„ç« èŠ‚æ‘˜è¦ã€‚"
    content = ["# ç« èŠ‚æ‘˜è¦"]
    # æŒ‰ç« èŠ‚æ ‡é¢˜ï¼ˆé”®ï¼‰æ’åº
    for title, summary in sorted(summaries.items()):
        content.append(f"## {title}\n\n{summary}")
    return "\n\n".join(content)

def _format_thematic_analysis_to_md(analysis: Dict[str, str]) -> str:
    """æ ¼å¼åŒ–ä¸»é¢˜åˆ†æä¸º Markdown"""
    if not analysis:
        return "æ²¡æœ‰å¯ç”¨çš„ä¸»é¢˜åˆ†æã€‚"
    content = ["# ä¸»é¢˜æ€æƒ³åˆ†æ"]
    for key, value in analysis.items():
        formatted_key = key.replace('_', ' ').title()
        content.append(f"## {formatted_key}\n\n{value}")
    return "\n\n".join(content)

def _format_debate_to_md(rounds: List[List[Dict[str, Any]]]) -> str:
    """æ ¼å¼åŒ–æ‰¹åˆ¤æ€§è¾©è®ºä¸º Markdown"""
    if not rounds:
        return "æ²¡æœ‰å¯ç”¨çš„è¾©è®ºè®°å½•ã€‚"
    content = ["# æ‰¹åˆ¤æ€§è¾©è®ºé—®ç­”"]
    for i, round_data in enumerate(rounds):
        content.append(f"## è¾©è®ºè½®æ¬¡ {i+1}")
        if isinstance(round_data, list):
            for item in round_data:
                question = item.get('question', 'N/A')
                answer = item.get('content_retrieve_answer', 'æ— å›ç­”')
                content.append(f"### é—®é¢˜: {question}\n\n**å›ç­”:** {answer}")
    return "\n\n".join(content)

def _format_draft_report_to_md(report_data: List[Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–æœ€ç»ˆæŠ¥å‘Šä¸º Markdown"""
    if not report_data:
        return "æœªèƒ½ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚"
    
    md_parts = []

    def _parse_recursive(section_list: List[Dict[str, Any]], level: int):
        for section in section_list:
            title = section.get("title", "æ— æ ‡é¢˜")
            md_parts.append(f"{'#' * level} {title}")

            content_brief = section.get("content_brief")
            if content_brief:
                md_parts.append(f"_{content_brief}_")
            
            written_content = section.get("written_content")
            if written_content and isinstance(written_content, list):
                md_parts.append("\n\n".join(written_content))
            
            children = section.get("children")
            if children:
                _parse_recursive(children, level + 1)

    _parse_recursive(report_data, 1)

    return "\n\n".join(md_parts)


def save_results(output_dir: Path, final_state: Dict[str, Any]):
    """å°†æœ€ç»ˆçŠ¶æ€å’Œæ ¼å¼åŒ–åçš„æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶"""
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\n--- æ­£åœ¨ä¿å­˜ç»“æœè‡³: {output_dir} ---")

    # 1. ä¿å­˜å®Œæ•´çš„æœ€ç»ˆçŠ¶æ€
    final_state_path = output_dir / "final_state.json"
    try:
        # TypedDict è½¬ dict
        serializable_state = dict(final_state)
        with open(final_state_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_state, f, ensure_ascii=False, indent=4)
        print(f"âœ… å®Œæ•´çŠ¶æ€å·²ä¿å­˜: {final_state_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å®Œæ•´çŠ¶æ€å¤±è´¥: {e}")
        print("--- æœ€ç»ˆçŠ¶æ€å†…å®¹ (pprint): ---")
        pprint(dict(final_state))

    # 2. æ ¼å¼åŒ–å¹¶ä¿å­˜ Markdown æ–‡ä»¶
    report_map = {
        "chapter_summary.md": (_format_summaries_to_md, final_state.get("chapter_summaries")),
        "draft_report.md": (_format_draft_report_to_md, final_state.get("draft_report")),
        "thematic_analysis.md": (_format_thematic_analysis_to_md, final_state.get("thematic_analysis")),
        "debate_questions.md": (_format_debate_to_md, final_state.get("raw_reviewer_outputs"))
    }

    for filename, (formatter, data) in report_map.items():
        output_path = output_dir / filename
        try:
            if data is not None:
                md_content = formatter(data)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(md_content)
                print(f"âœ… å·²ç”ŸæˆæŠ¥å‘Š: {output_path.name}")
            else:
                print(f"â„¹ï¸ æ— æ•°æ®å¯ç”¨äºç”Ÿæˆ: {output_path.name}")
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Š {filename} å¤±è´¥: {e}")


# --- 7. ä¸»ç¨‹åº ---
async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # è·å–ç”¨æˆ·è¾“å…¥å¹¶ç»´æŠ¤ä¼šè¯
    session_defaults = load_session_cache()
    user_inputs = get_user_inputs(session_defaults)
    save_session_cache(user_inputs)
    
    document_path = Path(user_inputs["document_path"])
    
    # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œè½¬æ¢å¤„ç†
    try:
        raw_markdown_content = convert_document_to_markdown(str(document_path))
        logging.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(raw_markdown_content)}")
    except Exception as e:
        logging.error(f"âŒ æ–‡æ¡£è½¬æ¢å¤±è´¥ '{document_path}': {e}")
        return

    # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„çº¿ç¨‹ID
    thread_id = hashlib.md5(str(document_path.resolve()).encode()).hexdigest()
    config = {
        "configurable": {"thread_id": thread_id},
        "recursion_limit": 50000  # æé«˜é€’å½’é™åˆ¶ä»¥å¤„ç†é•¿æ–‡æ¡£
    }

    print(f"\n--- ä»»åŠ¡ä¿¡æ¯ ---")
    print(f"æ–‡æ¡£: {document_path.name}")
    print(f"ä»»åŠ¡ID: {thread_id}")
    
    final_state = None
    # ä½¿ç”¨ async with æ¥æ­£ç¡®ç®¡ç†å¼‚æ­¥ checkpointer çš„ç”Ÿå‘½å‘¨æœŸ
    async with AsyncSqliteSaver.from_conn_string(str(CHECKPOINTER_DB_PATH)) as memory:
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä»»åŠ¡
        continue_task = False
        try:
            existing_state = await memory.aget_state(config)
            if existing_state and existing_state.next:
                print("\nâš ï¸ æ£€æµ‹åˆ°è¯¥æ–‡æ¡£æœ‰æœªå®Œæˆçš„ä»»åŠ¡ã€‚")
                choice = input("æ˜¯å¦ä»ä¸Šæ¬¡æ–­ç‚¹å¤„ç»§ç»­? (Y/n): ").lower()
                if choice == 'y' or choice == '':
                    continue_task = True
                    print("â–¶ï¸ æ­£åœ¨æ¢å¤ä»»åŠ¡...")
                else:
                    print("ğŸ—‘ï¸ å·²é€‰æ‹©å¼€å§‹æ–°ä»»åŠ¡ï¼Œæ—§è¿›åº¦å°†è¢«è¦†ç›–ã€‚")
            elif existing_state and not existing_state.next:
                 print("\nâ„¹ï¸ æ£€æµ‹åˆ°è¯¥æ–‡æ¡£å·²æœ‰ä¸€ä¸ªå®Œæˆçš„ä»»åŠ¡ã€‚å°†å¼€å§‹ä¸€ä¸ªæ–°ä»»åŠ¡ã€‚")
        except Exception:
            # å¯èƒ½æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œè¡¨ä¸å­˜åœ¨ç­‰
            print("\nâ„¹ï¸ æœªæ£€æµ‹åˆ°å†å²ä»»åŠ¡ï¼Œå°†å¼€å§‹ä¸€ä¸ªæ–°ä»»åŠ¡ã€‚")

        # ç¼–è¯‘å›¾ï¼Œå¹¶ç›´æ¥å…³è” checkpointer
        app = create_deepreader_graph().compile(checkpointer=memory)
        
        if continue_task:
            # åœ¨æ¢å¤å‰ï¼Œå¼ºåˆ¶æ›´æ–°æ£€æŸ¥ç‚¹ä¸­çš„æ–‡æ¡£å†…å®¹ï¼Œç¡®ä¿ä»»åŠ¡çš„å¥å£®æ€§
            try:
                print("â„¹ï¸ ä¸ºç¡®ä¿ä»»åŠ¡èƒ½æ­£ç¡®æ¢å¤ï¼Œæ­£åœ¨æ›´æ–°æ£€æŸ¥ç‚¹ä¸­çš„æ–‡æ¡£å†…å®¹...")
                await memory.aupdate_state(
                    config,
                    {"raw_markdown_content": raw_markdown_content}
                )
                print("âœ… æ£€æŸ¥ç‚¹æ›´æ–°æˆåŠŸã€‚")
            except Exception as e:
                print(f"âš ï¸ æ›´æ–°æ£€æŸ¥ç‚¹å¤±è´¥: {e}ã€‚å°†å°è¯•ç›´æ¥æ¢å¤ï¼Œä½†å¯èƒ½å‡ºé”™ã€‚")
            
            # ä»æ–­ç‚¹æ¢å¤
            async for event in app.astream(None, config=config):
                pprint(event)
                print("-" * 40)
        else:
            # å¼€å§‹ä¸€ä¸ªæ–°ä»»åŠ¡
            initial_state = DeepReaderState(
                user_core_question=user_inputs["user_core_question"],
                research_role=user_inputs["research_role"],
                document_path=str(document_path),
                db_name=str(CACHE_DIR / f"{document_path.stem}_{thread_id}.db"),
                # å…¶ä»–å­—æ®µç”±å›¾å¡«å……
                raw_markdown_content=raw_markdown_content,
                document_metadata={},
                table_of_contents=None,
                reading_snippets=None,
                snippet_analysis_history=[],
                active_memory={},
                chunks=[],
                chapter_summaries={},
                marginalia={},
                entities=[],
                entity_relationships=[],
                synthesis_report="",
                rag_status=None,
                raw_reviewer_outputs=[],
                report_narrative_outline=None,
                thematic_analysis=None,
                critic_consensus_log=[],
                final_keys=None,
                final_report_outline=None,
                draft_report=None,
                reading_completed=None,
                error=None
            )
            async for event in app.astream(initial_state, config=config):
                pprint(event)
                print("-" * 40)

        print("\n--- âœ… å›¾æµç¨‹æ‰§è¡Œå®Œæ¯• ---")

        # è·å–æœ€ç»ˆçŠ¶æ€
        try:
            final_snapshot = await app.aget_state(config)
            if final_snapshot:
                final_state = final_snapshot.values
                print("âœ… æˆåŠŸä»æ£€æŸ¥ç‚¹æ¢å¤æœ€ç»ˆçŠ¶æ€ã€‚")
            else:
                print("âŒ æœªèƒ½è·å–æœ€ç»ˆçŠ¶æ€ã€‚")
                # åœ¨ with å—å†…éƒ¨ï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥ return
        except Exception as e:
            print(f"âŒ è·å–æœ€ç»ˆçŠ¶æ€æ—¶å‡ºé”™: {e}")

    # åˆ›å»ºè¾“å‡ºç›®å½•å¹¶ä¿å­˜ç»“æœ (åœ¨ with å—ä¹‹å¤–)
    if final_state:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = BASE_OUTPUT_DIR / f"{timestamp}_{document_path.stem}"
        
        save_results(output_dir, final_state)
    else:
        print("æœªè·å–åˆ°æœ€ç»ˆçŠ¶æ€ï¼Œæ— æ³•ä¿å­˜ç»“æœã€‚")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åºã€‚")
        sys.exit(0)
